北京时间 2019.07.02 21:43 左右的时候，我的服务器监控突然检测到网站出现 502 错误，一开始我以为是网站日常被 CC 然后 php-fpm 挂了，在出现 3 次 502 后服务器会自动重启 php-fpm，但是重启完依然 502。

![img](https://i.natfrp.org/141248ef0d8fde557ac0dbc245301575.png)

我就发觉这事情不简单，一般来说重启完 php-fpm 就没事了，于是访问了一下自己网站，卡了半天，最后出现了一个 Nginx 样式的 502 错误页，只不过底部写着 Cloudflare。

![img](https://i.natfrp.org/cebc9a99ae838103956ede2eeccae1ce.png)

第一次见到这种奇怪的 502 错误页，于是下意识访问了一下 V2EX，结果发现 V2EX 也出现了这个错误页，再看了下 Telegram 群，哦豁，果然是 Cloudflare 翻车了。

当时就连 Cloudflare 的官网都挂了，下图为我本机和美国 VPS 的测试结果，都是 502 错误。

![img](https://i.natfrp.org/e52ef965df5eac20a8526a916b6877c7.png)

并且他们家的 DNS 1.1.1.1 也炸了

![img](https://i.natfrp.org/dfafb7a204bb3dc430286f332de723d6.png)

后来访问 Cloudflare 官网变成了这样

![img](https://i.natfrp.org/d1f9bcef8dfa24e24741fec5f222fab9.png)

这次 Cloudflare 足足挂了接近一个小时，官方说是挂了半个小时，导致挂掉的原因不是被攻击，而是因为他们最新部署的代码有错误：https://blog.cloudflare.com/cloudflare-outage/

这次看来 Cloudflare 要损失不少钱了……

### 官方博客原文翻译

今天 Cloudflare 网站的访问者收到了502错误，持续了大约 30 分钟，这些错误是由我们网络上的 CPU 利用率大幅上升引起的。此 CPU 峰值是由回滚的错误软件部署引起的。回滚后，服务恢复正常运行，使用 Cloudflare 的所有域名恢复到正常流量级别。

这不是攻击（正如一些人猜测的那样），我们对这一事件的发生感到非常遗憾。内部团队正在开会，因为我写了一个完整的检查报告，以了解这是如何发生的，以及我们如何防止这种情况再次发生。

从今天的 UTC 时间 1342 开始，我们在整个网络中遇到全球中断，导致 Cloudflare 代理域名的访问者显示 502 错误（“Bad Gateway”）。导致此次中断的原因是在新 Cloudflare WAF 托管规则的例行部署期间，在 Cloudflare Web 应用程序防火墙（WAF）中部署了一个配置错误的规则。

这些新规则的目的是改进阻止攻击中使用的内联 JavaScript。这些规则以模拟模式部署，其中新规则识别并记录问题但实际上没有阻止客户流量，因此我们可以测量误报率并确保新规则在部署到线上时不会导致问题产生。

不幸的是，其中一条规则包含一个正则表达式，导致 CPU 在我们全球的机器上飙升至 100％。这 100％ 的 CPU 峰值引起了客户看到的 502 错误。最糟糕的流量下降了 82％。

此图表显示了我们的一个 PoP 中的 CPU 峰值：

![img](https://i.natfrp.org/31dd9528da01dccb3e8075398e21e063.png)

我们看到了前所未有的 CPU 耗尽事件，这对我们来说是新颖的，因为我们之前没有经历过全球 CPU 耗尽。

我们在整个网络中不断进行软件部署，并拥有自动化系统来运行测试套件和逐步部署以防止事件发生的过程。不幸的是，这些 WAF 规则一次性全球部署，导致今天的宕机事故。

在 UTC 时间 1402，我们了解发生了什么，并决定在 WAF 管理规则集上发出 “全局查杀”，立即将 CPU 恢复正常并恢复网络。那发生在 UTC 时间 1409。

然后我们继续审查违规拉取请求，回滚特定规则，测试更改以 100％ 确保我们有正确修复了问题，并在 UTC 时间 1452 重新启用 WAF 管理规则集。

我们认识到这样的事件对我们的客户来说非常痛苦。在这种情况下，我们的测试流程不足，我们正在审查并更改我们的测试和部署流程，以避免将来出现此类事件。

